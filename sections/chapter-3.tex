%
\section{Literature Survey}\label{sec:literature survey}
%
This chapter provides brief information about the literature surveys specific to Data scraping and vulnerability analysis. All these existing surveys are particular about specific implementations. However, this implementation combines multiple methods to attain a complete automated OSS component extraction and analysis of each component.
%
\subsection{Evolution of Vulnerability Analysis}
Vulnerability analysis is an essential operation to perform in all domains, which also includes the software product. One vulnerability can cause a catastrophe in an active system and be an open gateway to the attackers to exploit the system. A good vulnerability assessment should define, identify and categorize the issues in the component\cite{LinRo2021}. 
Neumann and Parker say that the new vulnerabilities of IT systems are evolved from the attacks that happened in the past by using long-known techniques \cite{NePa1989}. Therefore building a solid and secure application is merely an impossible task or will be more expensive. Earlier the vulnerability assessment happens manually in all organizations where this is huge and overwhelming work for the IT security teams. 

All the newly identified vulnerabilities should be stored in excel or a CSV file for later use. When compared to the no. of vulnerabilities today, the ’90s and late 2000 had significantly less number of vulnerabilities \cite{Rh2019}. The early vulnerability scanning software gives a simple report of found vulnerabilities in the system. Later, this report has to be given to the IT security team to analyze the possible threats of the vulnerability. Then the report is sent to a higher authority for review and approval. It is such a manual process that has been used earlier to detect vulnerabilities in an active system \cite{Rh2019}. Manual scanning and repair strategies would soon become impractical as the number of vulnerabilities grew in the following years and the necessity of vulnerability management became more apparent to companies. Now the future of vulnerability analysis is focusing on fully automated assessment.
\subsubsection{Benefits}
Lutz Lowis describes that attackers may usually repeat their exploits by reusing a susceptible service[20]. Despite many old and new threats, there are few advantages to performing vulnerability analysis operations. Here are some advantages that can be achieved through a vulnerability assessment [\cite{LoAc2011}, \cite{Rh2019}, \cite{VulTest}]:

\textbf{Identify All Vulnerabilities:} A vulnerability analysis can identify all the possible vulnerabilities in the system. It can be identified by both organization and the attacker(hacker) if they use the same software for scanning. The organization has a higher advantage by fixing this issue before the attackers initiate. 
	
\textbf{Regular Scanning:} If the IT security person conducts regular system scanning, the scanning can give the level of risk available in the system. So this helps to figure out the health of the overall system.
	
\textbf{Frequent Assessment: } Vulnerability assessment will save money and time because if an organization or individual fails to complete the vulnerability analysis procedure, there is a greater possibility that an attacker will be able to exploit the system, resulting in the system having to be rebuilt.
	
\textbf{Past Reports:} The previous evaluation reports will help improve the present system in the future.

\subsubsection{Vulnerability Attacks}
A vulnerability is an attribute of a software component that can exploit or damage an active system. Most of the vulnerabilities have a high tendency of causing damage to security policy by internal or external persons(hacker or insider). There is a past event that happened in 2017 where a significant data breach occurred in Equifax. This data breach caused more than 100 million user data to be leaked \cite{Gilad}. In this vast area of IT, there are still new and unknown vulnerabilities emerging every day.

Ryohei Koizumi and Ryoichi Sasaki have found that whenever a vulnerability assessment operation takes place, the IT security team should always use the latest scanning tool because sometimes the old software is effective against some minor virus threat. However, it does not exploit the vulnerability \cite{KoSa2015}. Some vulnerability attacks have to be focused more on because these types of attacks are listed as high in risk level. Here are some major attack types that are focused on by few researchers \cite{AlAl2015}:

	\textbf{Configuration-based:} This type of vulnerability occurs when a misconfiguration in the system or running any unwanted services in the background. The configuration-based vulnerability has a central weak point. The hackers can quickly get into the organization network and find the active system by any form of misconfiguration. This type of vulnerability is based on weak management protocols, weak permissions and weak encryption.
	
	\textbf{Security patches:} A security patch is an essential update for all active systems because the role of security patches is to fix or remove the flaw or issue found from the vulnerability report. The software patches play a vital role in rectifying and fixing the components for commercial and open-source software components. Generally, the security patches will be released every month; for example, Microsoft sends security patches to its Windows operating system every month. A system that fails to update its security patches will lead to significant security breaches. As everyone knows, the best example of security patches failures is ransomware which is called wannacry \cite{Kaspersky}.
	
	\textbf{Zero-Day Vulnerability:} It is the most challenging vulnerability to rectify and fix the issue. It is because the vulnerability is new and unknown to the organization. The attackers will take advantage of this loophole and will cause a significant security risk. The term “Zero-day” is used because the vendor was unaware of the threat which affected the software, and the vendors had “0” days worked on the security patch or fixing the vulnerability.
	
	\textbf{Faulty Open-Source Package:} It is the vulnerability where hackers have been using for several years. Generally, the hackers used to inject a credential sniffer inside an instrumental shared library or package. This kind of attack happens when the vendor does not update the open-source packages regularly. This threat will stay in the package for several days or until the vendor finds it.


\subsubsection{Types of Vulnerability Analysis}
The vulnerability analysis is also called vulnerability assessment, where the primary purpose is to keep the organization safe from digital threats. It is a methodology that is used to find the IT application and the infrastructure. It also involves intense scanning by the security expert or team of the organization. Few types of vulnerability analysis are used exclusively for some parts of IT:

{\bf Network-based Analysis:} A network-based analysis is a mechanism to identify the network defects and issues in the network. They mostly scan and analyze the network endpoint and device network for security issues. The failure of vulnerability analysis will allow the hackers to take advantage of the network issue. The organization will invest more time to improve its existing framework, which is used for network vulnerability analyses. In 2008 Hai L Vu etl developed a vulnerability analysis framework for scanning network vulnerabilities, and along with that, they have also proposed a scalable algorithm. Both framework and algorithm are used to evaluate the network vulnerabilities without generating a full-scale graph \cite{VuKhChFe2008}. The table ~\ref{tab:networkanalysis} gives the results of each network component used in the network by using the framework. The results come with a brief description of the effects of the listed vulnerability. This information about the vulnerabilities have been taken with the help of vulnerability databases \cite{VuKhChFe2008}.
\begin{table}[H]
	\begin{center}
		\resizebox{\textwidth}{!}{
		\begin{tabular}{|c|c|c|}
		\begin{tabular}{p{2cm} | p{12cm} | p{10cm}}
			\hline
			Network Components & Nessus Vulnerability Output & Vulnerability \\
			\hline
			%1
			SIP Proxy($H_1$) & {\bf CVE Reference: CVE-2005-0449}
			
			{\bf Description:} Allows attacker to bypass net filer/iptables or
			initiate a denial of service attack. 
			
			CVSS Base Score: 5.0
			
			Pre-Condition: Linux, Kernel 2.6.x
			
			Post-Condition: Denial of Service, bypassing firewall
			& V (source, $H_1$, 0.50, Linux-kernel-2.6.1, bypassing\_firewall) \\ 
			
			%2
			& {\bf CVE Reference: CVE-2008-1483}
			
			{\bf Description:} Attackers are able to hijack a SSH Session
			
			CVSS Base Score: 6.2
			
			Pre-Con OpenSSH-4.3p2 and earlier
			
			Post-Condition: Allows remote connection, Gain access to system.
			& V (source, $H_1$, 0.62, OpenSSH-4.3p2, hijack\_session)  \\ 
			
			%3
			Asterisk Server ($H_2$)
			& {\bf CVE Reference: CVE-2008-1483}
			
			{\bf Description:} Attackers are able to hijack a SSH Session
		
			CVSS Base Score: 6.2
			
			Pre-Condition: OpenSSH-4.3p2 and earlier
			
			Post-Condition: Allows remote connection, Gain access to system.
			& V (source, $H_2$, 0.62, OpenSSH-4.3p2, hijack\_session)  \\ 
			
			%4
			& {\bf CVE Reference: CVE-2008-0095}
			
			{\bf Description:} Allows remote attackers to cause Denail of Service to Asterisk via a BYE message with an Also header.
			
			CVSS Base Score: 5.0
			
			Pre-Condition: Asterisk, open source, 1.4.0 and previous.
			
			Post-Condition: Denial of Asterisk/VoIP services.
			& V (source, $H_2$, 0.50, Asterisk 1.2.1, DoS)  \\ 
			
			%5
			& {\bf CVE Reference: CVE-2007-1306}
			
			{\bf Description:} Allows remote attackers to cause Denial of Service to Asterisk using a SIP packet without a SIP-version header.
			
			CVSS Base Score: 7.8
			
			Pre-Condition: Asterisk, open source, 1.4.16 and previous.
			
			Post-Condition: Denial of Asterisk/VoIP services.
			& V (source, $H_2$, 0.78, Asterisk 1.2.1, DoS)  \\ 
			\hline
		\end{tabular}
	\end{tabular}
	}
		\caption{Sample results of the network vulnerability analysis framework \cite{VuKhChFe2008}.}\label{tab:networkanalysis}
	\end{center} 
\end{table}
\newpage
{\bf Host-based Analysis:} Sometimes a vulnerability can be found in the vendor's resources, and with this vulnerability, there are high chances where even an insider can be an attacker for the system. The attackers mainly cause damage by making an improper configuration setting in the host. This vulnerability assessment takes place in servers, workstations or other network hosts. The host-based analysis will give a detailed insight into configuration settings in the network, patches and update history. The insight gathered from the analysis will also give us the potential damage caused by the attackers or intruders. Anil Sharma et al. created a software tool, "Ferret", by using Perl language that identifies the vulnerabilities present in the host \cite{CuAnSaMaSh2004}. This software tool helps the system administrator to identify the vulnerabilities and take action based on the threat. The host vulnerability is checked using a different plug-in module, and the end output will also mention which plug-in module is used for the assessment.

{\bf Database Analysis:} Misconfigurations often occur in databases and Big Data systems. Database vulnerability analysis is mainly used for identifying the known risk in the databases. The most common risks are missing patches, weak passwords and default vendor accounts \cite{Im2021}. Sartaj Singh described the importance of inherent dangers of the database, like how data theft is happening in the internet era, and he also mentioned that the existing encryption methods are not foolproof for the high-end professionals[31]. A vulnerability attack can exploit file permission and database configuration files and can potentially steal sensitive information like credit card details, personal details, etc. There is still research going to secure the database more effectively and efficiently. In 2008 Ghassan Jabbour and Daniel A. Menasce presented a framework that provides self-protection to the database from unauthorized or intensional security parameter changes. Also, they proposed that this framework can be implemented in an Oracle 10g Release 2 database \cite{JaMe2008}.

{\bf Application Analysis:} Vulnerabilities are frequently identified in third-party apps that are built and managed. The vulnerability assessment is essential to an organization’s security team because they have to identify the vulnerability in the application before it exploits the system. This process is used to identify misconfiguration vulnerabilities in applications, outdated software packages and weak authentication. Sultan S. Alqahtani \cite{Al2017} has researched a modelling approach that improves traceability and trust in software products by linking the security knowledge with the software artefacts. He also introduced a scanner called  Semantic Global Problem Scanner(SE-GPS), which is created by integrating the modelling approach. With the modelling approach, the tool can now link the \acs{NVD} \cite{nist} security database to the maven build repository. 

The application vulnerability analysis is a crucial security process to be considered by the organization. The process's primary goal is to identify the vulnerability and report it to the security authority to mitigate it before the vulnerability exploits the system. Most application vulnerability analysis tools use the proper guidelines to make a good scanning tool. Here is the main guideline that a good vulnerability scanner tool should use \cite{Syamini}:

	\textbf{Setup:} The setup should begin with proper documentation about the application, with perfect security permissions and configuration tools.
	
	\textbf{Test Execution:} Run the scanner tool, which can be an existing tool or a self-created one. The scanner should identify all the software packages and their dependencies used inside the software product.
	
	\textbf{Vulnerability Analysis:} Once the execution is finished, the extracted software packages and their dependencies should be analysed by using any existing vulnerability databases like \acs{NVD}, ISS(Internet Security Systems), etc. Each software package is searched in the database by using its name and version.
	
	\textbf{Reporting \& Remediation:} After the previous process, the analysis will give a formal report of each software package. The report includes the risk level of each package which is categorized as LOW, MEDIUM and  HIGH. Sometimes the report will also provide a brief description of the threat. The security department of the organization takes the remediation action. Mostly, the remediation will have two possibilities: to change the version of the package or find an alternate software package.

\subsubsection{Vulnerability Databases}
A vulnerability database is a collection of information about all known defects of a software application. This database is aimed to maintain all the information like name, version, threat description, level of risk and history of the components. New vulnerability information is generated every day to be fed into the database to be a known vulnerability for future users. The respective vulnerability database adequately reviews each information before being generated into the database \cite{Va2017}. The software owner, security researcher, and public users of the software can give the vulnerability information. Most of the databases use \acs{CVE}’s information to be integrated with the vulnerability database[14]. The Common Vulnerabilities and Exposures(\acs{CVE}) is a program that the MITRE’s corporation owns. The \acs{CVE} is built for one primary purpose, which is to identify, describe and catalog cybersecurity vulnerabilities.

There are a lot of public and private vulnerability databases which both government and private organizations maintain. Each database is selected based on the user's choice, where some like to go with public or proprietary databases. In most cases, the user always prefers to use the public database for two main reasons, it is maintained by the government and its open-source. Here is a list of significant vulnerability databases \cite{LoAc2011}:

{\bf NVD:} Among all the databases National Vulnerability Database is the most commonly used database by users. The \acs{NVD} was first found in 2005 by the US National Institute of Standards and Technology(NIST) \cite{nist}. Most of the known vulnerabilities for both commercial and open-source are fed into the \acs{NVD} databases, and that is why \acs is one of the largest and most efficient vulnerability databases. \acs{NVD} is a completely open-source project so that anybody can use it without any restrictions. The \acs{NVD} integrates \acs{CVE}’s information for every component present inside the database \cite{cve}. The \acs{CVE} is mapped in the database by using the \acs{CVE} id so that the user can easily retrieve the information from the \acs{CVE} dictionary if needed. All the data stored in the database have a unique id for each vulnerability, history date, and short vulnerability description. The \acs{NVD} also integrates \acs{CVSS}(Common Vulnerability Scoring System) information to each vulnerability.

The figure ~\ref{fig:nvd} will give a clear idea about how components can be searched in the \acs{NVD} database. The result list will be provided based on the keyword entered in the search bar along with information like \acs{CVE} id, a summary of the vulnerability and \acs{CVSS} score.
\newpage
\begin{figure}[H]
	\includegraphics[width=15cm]{includes/nvd.png}
	\centering
	\caption{\acs{NVD} result for “jquery” component \cite{nist}.}
	\label{fig:nvd}
\end{figure}

{\bf SecurityFocus:} The SecurityFocus is a traditional news portal where the community discusses the new security issues of the vulnerabilities. The Bugtraq is the most common product of the SecurityFocus. The Bugtraq tool is exclusively used to list all security issues, like new vulnerabilities, exploitation methods and security related announcements by the Vendor. Bug traq was created in 1993 by Scott Chasin. On Tuesday, May 14th, 1996, Aleph One gained control of BugTraq. BugTraq has evolved into a well-respected security mailing list with over 27,000 subscribers over the years \cite{BugTraq}.

Ashish Arora \cite{AsRaRaYu2006} made an empirical analysis on Vendor’s patching behaviour by evaluating the gap between the date of vulnerability disclosed and the date of security patch rollout. They have discovered that a vendor takes an average of 29 days to fix the issue from the date of vulnerability is disclosed. To achieve this, the authors have compiled data from SecurityFocus and CERT/CC. The author also confirmed that the vendors take more time to fix the vulnerabilities they get from CERT/CC. It means the authors say that the Vendor responds faster to SecurityFocus vulnerabilities than the CERT/CC. The results of SecurityFocus will give information like Bugtraq ID, and it is class of error, publish date and its \acs{CVE} ID. It also uses \acs{CVE} information to list all vulnerabilities and give a discussion forum to report any other errors after the security patches. Figure ~\ref{fig:securityFocus} is an example of the vulnerability publication of SecurityFocus.
\newpage
\begin{figure}[h!]
	\includegraphics[width=15cm]{includes/securityFocus.png}
	\centering
	\caption{Example of vulnerability publication by SecurityFocus \cite{SecurityFocus}.}
	\label{fig:securityFocus}
\end{figure}

{\bf IBM X-Force:} The IBM X-Force is an elite team of security experts that contains hackers, and incident responders \cite{IbmXforce}. They are dedicated to resolving some of the toughest cybersecurity challenges in the world. The IBM X-Force is led by former FBI Cybercrimes Division Assistant Director Chris Soghoian, who brings over 20 years of experience to this role. Unlike \acs{NVD}, the X-force is proprietary software, whereas NV is completely open-source. Despite being proprietary software, the X-force gets its vulnerabilities information from the \acs{CVE} dictionary. Apart from this, X-force is also providing API services for this software. X-force requires an IBM id to get full access, which will be a paid service later. X-force also uses \acs{CVSS} scoring system to give a clear risk level of the vulnerability. Though it is a proprietary database, not many researchers will use this database for research purposes. Figure ~\ref{fig:ibm} is an example of vulnerability publication by IBM X-Force \cite{IbmXforce}.
\newpage
\begin{figure}[H]
	\includegraphics[width=15cm]{includes/ibm.png}
	\centering
	\caption{Example of vulnerability publication by IBM X-Force \cite{IbmXforce}.}
	\label{fig:ibm}
\end{figure}

{\bf CERT/CC:} The \acs{CERT/CC} stands for Computer Emergency Response Team Coordination Center \cite{Web2021}. \acs{CERT/CC} is also one of the databases which provide information about ongoing vulnerabilities, and it also tries to resolve incidents like data breaches and denial-of-service attacks. \acs{CERT/CC} was founded in 1988 by Carnegie Mellon University in Pittsburgh, Pennsylvania and supported by the Defense Advanced Research Projects Agency, which was part of the U.S. Department of Defense \cite{CertDiv}. The main characteristic of \acs{CERT/CC} is to resolve the security incident and try to regain control and minimize the damage. Later on, they will assist in reporting the incident response to prevent the issue from happening again. Figure ~\ref{fig:cert} is an example of vulnerability publication by \acs{CERT/CC}.
\newpage
\begin{figure}[H]
	\includegraphics[width=15cm]{includes/cert-cc.png}
	\centering
	\caption{Example of vulnerability publication by \acs{CERT/CC} \cite{CertDiv}.}
	\label{fig:cert}
\end{figure}

In general, the main goal of the incident response team is to protect the organization from any vulnerability, which can be software, network or cybersecurity-related issues. The \acs{CERT/CC} follows a universal incident response model, which is “protect, detect and respond” \cite{DiWhGo2004}.

	\textbf{Protect:} The first step of this model is to protect the organization by taking some security measures before the incident happens. This model focuses on more protection than reacting to it, such as creating an incident response plan, providing security awareness, performing risk analysis, etc.
	
	\textbf{Detect:} It is a process that happens before responding to the vulnerabilities. A proper response can be given to solve the vulnerability if the detection of the vulnerability is proper. Sometimes the detection duration might take a month, week or a day, and it entirely depends on the security incidents. Before performing a detection strategy, the responsible person should provide solutions, for example, applications that often run, how regular network traffic looks, what the network protocol should be avoided, etc. The standard technique of detection is routers, firewalls, network monitors, etc.
	
	\textbf{Respond:} Once the security incident is detected, the final step will be to provide the perfect solution. Generally, responding to the security incident has a few steps. The first step will be getting the information about the security incident from the vendor or business partner. The next step is for the team to analyse the information to find quick and practical solutions. Once the solution is found, the team will create an immediate strategy to stop the issue before it causes more damage. The last step will be responding to the incident and will be published to others so that the affected users can regain control.

\subsection{Data Scraping}
Data scraping is an automation process used to extract data from files, websites, databases or any application. With the help of data scraping, a user can get a considerable amount of relevant data such as product reviews, business contact information, social media post or specific content from a file. The content mentioned above can be extracted using existing data scraping tools or build a custom program with the help of programming language support. Sometimes data scraping is also called web scraping just because it is widely used in the web domain. After the birth of the world wide web in 1989, businesses started showing interest in creating websites to show their business-related information such as product details, upcoming events and contact forms. Since then, the evolution of data scraping has been tremendous, whereas now, the data scraping techniques are used in cloud-based applications. Ram Sharan Chaulagain et al. proposed a cloud-based web scraper architecture that can handle storage and computing resources with elasticity by using Amazon web service \cite{SaBa2016}. This architecture was proposed concerning the previous drawbacks of scraping large amounts of data, such as reliability of data scraping, storage issue of extensive data and intensive computation. So, therefore, this clearly explains that large amounts of data cannot be extracted by using the traditional data scraping methods.

\subsubsection{Types of Data Scraping}
Generally, data scraping techniques are used in the area of the web or any enterprise application. Depending upon the data extraction requirements of the system, the data scraping is categorised into the following types: 

{\bf Web Scraping:} Web scraping is also called “web harvesting”, “web data extraction”, or even “web data mining”. Web scraping is an automatic process of getting the web data from a web page and parsing the data to get the required information to organize a separate database, and this is called web scraping. The main goal of web scraping is to lower the need for human involvement in downloading web pages, manually organizing the web data to a database or spreadsheet by using copy-paste technique. The automated process of web scraping is much efficient and cost-effective than manual web scraping. Apart from this, automated web scraping can be configured to have higher accuracy for data extraction than human accuracy. Web scraping came into the picture when the web was invented. Figure  ~\ref{fig:webscraping} shows the workflow of a standard web scraper. The Scrapehub is the part where it takes a URL as an input, and the given input is processed based on the client’s configuration. The scrape engine uses different libraries or self-made programs for parsing the web data and converts it into meaningful information to organize in the database \cite{SaBa2016}.

\begin{figure}[H]
	\includegraphics[width=15cm]{includes/webscraping.png}
	\centering
	\caption{ Traditional Web Scraper \cite{SaBa2016}.}
	\label{fig:webscraping}
\end{figure}
The process of web scraping is a combination of two operations. First, the web data extraction and next will be processing or cleaning the extracted raw data to provide insightful information. The extraction part is more accessible when the source data is in a database or an ontological structure. However, in most cases, all web data extraction deals with unstructured or semi-structured data. The researchers are coming up with new methods to improve the data scraping more efficiently and fast. In 2021 Jaebeom You \cite{YoLeKw2021} proposed a method called “DeepScrap” for collecting tweets. The DeepScrap method is used to scrape all the recent tweets quickly by using Twitter’s standard API. The multiprocessing of deepScrap helps to get refined information rather than going with single processing. This scraping technique can help the OSS scanner to extract the name and version of a component.

{\bf Screen Scraping:} In short, screen scraping means reading text data from a computer terminal screen or a piece of programming that mediates legacy applications, and modern user interfaces \cite{Alex2021}. Both web scraping and screen scraping have many similarities, except a few key differences, such as where the data is gathered and how it is used. This scraper technique is mainly used for scanning old sources because of the quick speed of technological change. Specific legacy systems, software, and applications become outdated and expensive to maintain. It would be a big and complicated process when the organization decides to migrate the old resource to the latest one without the help of a screen scraper. In 2017 there was a report where SnapLogic and the independent research firm Vanson Bourne stated that more than 500 U.S IT companies data are completely trapped in their legacy system \cite{Snap}. A \$140 million loss was incurred as a result of the data trap. Not every company needs to use screen scraping. It is more needed for companies like companies that hold their client's data for very long time record-keeping purposes like companies that produce CRM services, crypto or stock exchanges, etc. Even without the source code of any legacy application, the screen scraper can still extract the data. 

Sergio Flores-Ruiz et al. have explained black-box solution for the migration process of a mainframe by using screen scraping technique \cite{FlPeDoPu2018}. The author came up with a back-box solution to JavaFX and relational database mainframe systems because most of these legacy systems are consolidated on mainframes. The author also mentioned that the previous migrating solution was inefficient due to a shortage of systematicity and lack of business rule verification. As we know, applying the screen scraper technique to retrieve legacy information from an old source is a compelling idea, but some research has tried to take the screen scraping next step. An operation of screen scraper is, once the information is retrieved from the legacy information system and moved to the new data storage system. After this operation, the legacy information system will be deactivated by the organization. Alex van Oostenrijk \cite{Alex2004} has tried to implement a web service between a website and the legacy information system by using the screen scraper technique. He believed that a continuous screen scraping system would be essential to achieve it. Continuous screen scraping is nothing but keeping the old information system active and connecting the new system(website) with the help of web services. After the implementation, the author experienced that this is not a robust idea because it took four minutes to search 132 pages per request, but it can be overcome by using a caching technique.

\subsubsection{Challenges}
%
%
The usage of data scraping is expanding every day, and along with that, the challenges also travels with it. Data scraping helps extract information based on the requirements when consumed, but creating a data scraper is not an easy task. There are few challenges that have to be taken into consideration before creating a data scraper, and the listed challenges are considered for only data scraping [\cite{And2021},\cite{Mic2020}]:

{\bf Complicated Data Structure:} It means the structure of the data source can be varied from one another. This challenge will occur when a system is trying to extract data from multiple data sources. This can be resolved if the developer designs a scraper effectively by focusing on the target data source.

{\bf Scraping Time:} The challenge will be faced when the system is trying to extract significant data sources frequently. This may cause system overloading, and the scraping will be not effective. The system should maintain a proper balance in scraping time to overcome this issue, like avoid frequent data extraction and perform an extraction with time intervals.

{\bf Data Quality:} It is a significant challenge to focus on because data scraping is all about extracting meaningful information. The quality of data is evaluated by cross verifying with the predefined requirements. This challenge can be easily solved by making sure that all the predefined requirements match the validation process.

{\bf Legal Challenges:} This kind of challenge affects only web data scraping because most corporate or private websites will not accept any data scraping bot to extract their information without their knowledge. It does not mean that data scraping is illegal, but it may cause problems if it is not done with proper permission from the respected owners of the data source or website.

{\bf Large-scale Extraction:} One of the significant challenges in data scraping is that academics are currently working to improve the technology for large-scale data extraction. This issue leads to a breakdown if the data scraping is continuously being performed. In most cases, the data scraping techniques are used for minor operations.
 
%
%
\subsubsection{Data Scraping Methods}
Mostly data scraping used purposes like marketing and price research, monitoring, analyzing and retrieving information for decision making and CRM tools. If a user or organization configures the data scraping technique effectively, the tool will be pretty powerful for retrieving meaningful information. The data scraping has been classified into two scraping techniques which will be utilized based on the requirements:

{\bf Manual Scraping:} As everyone knows, data scraping is done chiefly automated but can also be manually scraped from a resource. It is the traditional way of data scraping. This process is nothing but seeing through the vision and replicating it into the desired format like a spreadsheet. The advantage of manual scraping is that it is straightforward to implement, with no additional skills and human intervention. At the same time, manual scraping also has its drawbacks. Compared to automated scraping, it is slow, high cost, and errors may occur with human intervention. Under manual scraping, there is only one technique available \cite{Radware}:

	\textbf{Copy-Pasting:} In Manual scripting, all the user wants to know is copy-pasting, which takes much effort. Most of the time, manual scripting will have data reputation issues because of human involvement. On the other hand, manual scraping is seldom seen in reality because automated scraping is considerably faster and less expensive.


{\bf Automated  Scraping:} The evolution of manual scraping is automated scraping, and the current IT era is moving through automation to reduce human effort and make it cost-effective with high accuracy. Because of their simplicity of use and time and cost advantages, automated web scraping technologies have grown in popularity. Automated scraping has more advantages than manual scripting like very fast data scraping and extraction, time and cost efficiency, easy to use and supports API services. Eventually, it also has its negative side, which is not that bad, like requiring light training; in some cases, scraping is illegal and lacks human checks. Automated scraping has many methods to scrape data based on the requirement \cite{Radware}:

\textbf{HTML Parsing:} It is the easiest and fastest way of extracting data from a file. This technique is mainly used for extracting data from HTML files. The parsing is done with the help of a programming language like JavaScript to extract data from the HTML file. 
	
\textbf{ DOM Parsing:} The document model object is an interface used to parse XML or HTML file source code into a string. The primary purpose of the DOM parser is to get the in-depth structure of the HTML or XML file. The representation of a document is shown in the tree view. With the help of a DOM parser, the scraper can extract the information from the node data. The positive side of the DOM parser is more effective than standard parsing methods like the data persisting in the memory, forward and backward traversing is possible, and immediate changes are possible.

\textbf{ XPath:} XPath stands for XML path, and it is an exclusive querying language for XML files. The xpath is used to navigate the XML file because the XML documents are based on the tree structure. There are few reasons why XPath is preferable when parsing XML files, like the queries are compact, easy to use, simple syntax, do not return repeated values and work with both HTML and XML attributes. Due to its interoperability, the XPath can be used in any programming language like C, C++, C-Sharp, Java, Javascript, etc. Xpath also can retrieve the relevant information from any complicated XML file by allowing different types of expressions. The essential expressions are Root, Element, Attribute, Text and Comment.

\textbf{ Text Pattern Matching:} Pattern matching is a powerful tool for extracting information from natural language. It can be used to perform tasks from simple text segmentation to complex parsing and machine translation. In technical terms, it is verifying the given sequence of characters to find similarities with a pattern. The patterns to verify the given string are designed or created with the help of regular expression. Most programming languages support regular expression. Unlike other methods, pattern matching can be implied on all data types. Pattern matching can be performed simultaneously with the help of parallel pattern matching. 

\subsection{String Distance Metrics}
Both supervised, and unsupervised learning can benefit from distance measures. Distance measurements can be used for text mining, medical analysis, document categorization, and similarity analysis, among other things. The primary purpose of string metrics is to correct a spelling mistake by calculating the distance between two data points or strings; the calculation will give the string similarity between the given inputs. In simpler terms, it calculates the distance between one text to another text to find similarities. The string metrics are frequently used as information integration in areas like fraudulent detection, fingerprint analysis, plagiarism detection, ontology merging, DNA analysis, RNA analysis, image analysis, etc. The distance between strings may be calculated using a variety of algorithms. Each algorithm calculates the distance between strings, but apart from this, it also has some unique features \cite{Wu2020}. The following algorithms are:
\subsubsection{Hamming Distance}
Hamming distance is a simpler algorithm to calculate the distance between two strings found by Richard Hamming, who introduced this technique for Hamming codes. The algorithm performs calculations to find the similarity in the given strings by comparing the changes in the position of the two strings \cite{Tok2015}. The equation of hamming distance will look like Equation 1, and the ~\ref{tab:hammingTable} illustrates the distance value of the words \cite{Saed2021}.
\begin{equation}
	\label{eq:hamming}
	\begin{aligned}	
	{D_H} = \sum_{i-1}^{k} \vert {x_i} - {y_i}\vert \\
	x = y\Rightarrow D = 0 \\
	x \neq y\Rightarrow D = 1
	\end{aligned}
\end{equation}

\begin{table}[h!]
	\begin{center}
		\begin{tabular}{ |c|c|c| } 
			\hline
			X & Y & Distance \\
			\hline
			Male & Male & 0 \\ 
			Male & female & 1 \\ 
			\hline
		\end{tabular}
		\caption{Distance between the word "Male" and "Female".}\label{tab:hammingTable}
	\end{center} 
\end{table} 

Equation 1 explains how the distance is calculated between the two strings to find the similarity. The benefits of using hamming distance are simpler and effective to detect spelling mistakes or errors. They are also quite good and effective in data streams. The only disadvantage of using Hamming distance is that the bandwidth usage is more.
\subsubsection{Levenshtein distance}
Levenshtein distance algorithm is also used to find the distance between two strings and a few operations. Vladimir Levenshtein from Russia discovered the algorithm. The Levenshtein distance is also known as the Edit-distance based algorithm because it computes the number of edits to find the distance. The edit contains three operations which are Insertion, Deletion and Substitution. To find less similarity between two strings, it requires more operation \cite{ChSa2019}. For example, the distance between GILY and GEELY is two, and two operations have been done: substitution and deletion. The equation of Levenshtein distance looks like Equation 2 \cite{Cuelogic}.

\begin{equation}
	\begin{aligned}
		\label{eq:levdist}	
		lev_{a,b}(i,j) = 
		%\min \left \{
		\begin{cases}
			\max(i,j) & \text {if } mini(i,j) = 0,\\    
			\min 
			\begin{cases}
				lev_{a,b}(i-1,j) + 1 \\
			    lev_{a,b}(i-1-j) + 1 & \text                     otherwise. \\ 
			    lev_{a,b}(i-1,j-1) + 1_{a_i\neq b_j} 	
			\end{cases}
		\end{cases}
	%\caption{ Levenshtein distance Formula \cite{Cuelogic}.}
	\end{aligned}
\end{equation}

The algorithm can be used for word suggestion and autocorrection by measuring how different two strings are by counting the number of character edits. Implementing the Levenshtein distance algorithm as a recursive implementation will cause massive complexity, but this can be solved by using a proper memorization technique.

\subsubsection{Damerau-Levenshtein distance}
The Damerau-Levenshtein distance algorithm is an updated version of the Levenshtein distance algorithm. The one difference which separates  Damerau-Levenshtein from Levenshtein is that it includes another operation: transposition. It takes only a minimum number of operations to find the similarity of the strings by changing one word to another. The name Damerau-Levenshtein was derived from the scientists Frederick J. Damerau and Vladimir I. Levenshtein. The main benefit of using the Damerau-Levenshtein algorithm is that it is comparatively faster than its predecessor algorithm, and it also saves time by avoiding Regex expression to calculate the similarity \cite{ChSa2019}. The equation of Damerau-Levenshtein looks like Equation 3 \cite{DamLeven}.

\begin{equation}
	\begin{aligned}
		\label{eq:dlevidist}	
		d_{a,b}(i,j) = 
		%\min \left \{
		\begin{cases}
			0 & \text {if } i = 0 j = 0\\
			d_{a,b}(i-1,j) + 1 & \text {if } i > 0\\
			d_{a,b}(i,j-1) + 1 & \text {if } j > 0\\
			d_{a,b}(i-1,j-1) + 1_{a_i\neq b_j} & \text {if } i,j > 0\\
			d_{a,b}(i-2,j-2) + 1_{a_i\neq b_j} & \text {if } i,j > 1 and a[i] = b[j-1] and a[i-1] = b[j]\\
		\end{cases}
		%\caption{ Damerau-Levenshtein Distance formula \cite{DamLeven}.}
	\end{aligned}
\end{equation}

In natural language processing, the Damerau–Levenshtein distance is a crucial factor. They are mainly considered in the area of DNA analysis and Fraudulent detection.
\subsubsection{Jaro Distance}
Jaro distance is a metric for comparing the similarity of two strings, and it is defined using Equation 4 \cite{JaroWinkler}.

\begin{equation}
	\begin{aligned}
		\label{eq:jaro}	
		d_{a,b}(i,j) = 
		%\min \left \{
		\begin{cases}
			0 & \text {if } m = 0\\
			\dfrac{1}{3}\left( \dfrac{m}{s_1} + \dfrac{m}{s_2} + \dfrac{m - t}{m} \right) & \text otherwise\\
			\
		\end{cases}
		%\caption{ Jaro similarity formula \cite{JaroWinkler}.}
	\end{aligned}
\end{equation}

Here s1 and s2 are the two strings. m is considered the number of matching characters, and t is considered half the number of matching characters. Unlike the other string distance algorithm, the Jaro distance ranges from 0 to 1, where 0 means no similarity and 1 has similarity \cite{Ak2021}. The result of the algorithm will be given as a float value. It can be used to check whether the two strings are the same or not. 
The improved version of the Jaro similarity algorithm is the Jaro-Winkler distance algorithm. The Jaro-Winkler algorithm is also similar to the Jaro similarity. However, the Winkler is built using prefix scale p for finding precise distance, and the equation looks like Equation 5 \cite{Stat2021}.

\begin{equation}
	\begin{aligned}
		\label{eq:jarowinkler}	
		Sim_{JW}(s_1, s_2) = Sim_J(s_1,s_2) + lp[1 - Sim_J(s_1.s_2)]
		%\caption{ Jaro-Winkler similarity formula \cite{Stat2021}.}
	\end{aligned}
\end{equation}
